== Team name == 

    Tech United Eindhoven

== Main ability == 

    Human assisted geometric, semantic mapping ...

== Test objective == 

	Show that the robot can interact with and manipulate newly learned dynamical(!) objects by simply giving them a name. 

== Abilities used == 

    [ ] Audio Processing
    [ ] Artificial Intelligence
    [?] Computer Vision
    [ ] Endurance / Strength
    [ ] Human Robot Interaction
    [ ] Learning
    [X] Manipulation
    [X] Navigation
    [X] Other: world modeling / (semantic, geometric) mapping

== Novelty and Scientific / League Contribution ==

    - Peform tasks without needing hard-coded environment knowledge besides the location of the walls.
    - Incremental learning / segmentation bootstrapping (the more you know, the easier it gets).
    - Automatically find the best pose to approach an object.
    - This experiment can instantly be repeated:
		* for every furniture layout of this arena.
		* for all objects (within detectable resolution)
 

== Test description == 

	AMIGO enters a new environment of which only the wall locations are known. A human operator uses a gui to send AMIGO around the room. All unknown objects can be labeled by the operator by simply clicking them and giving them a name. These objects are automatically extracted from the depth data and a 3D models is generated and added to the world model. Even adding unknown objects on top of (now) known objects is no problem (e.g. a cup on a table).
	
	Now to demonstrate this ability the operator sends AMIGO to locations by voice commands (e.g. drive to table). Using a constrained-based planner the best location to approach this object is determined (note the blue arrows on the screen).
	
	It is demonstrated that moving an object that was previously added to the world model is tracked by AMIGO.
	
	The demonstration ends with a request to AMIGO to grab one of the learned objects and put it in the trash bin. Note that neither the object nor the thrash bin was known to AMIGO upon entering the room. 
